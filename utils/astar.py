import heapq
import os
from typing import List

import pandas as pd

from procgen_wrapper.action_space import LeaperAction, ProcgenAction
from procgen_wrapper.extended_state import ExtendedState
from procgen_wrapper.procgen_simulator import ProcgenSimulator, LEAPER_UP_PENALTY, LEAPER_STEP_PENALTY
from utils.bfs import BFSNode
from utils.hash_2d_array import hash_2d_array

NUM_STEPS_PER_UP_ACTION = 3
LEAPER_DATA_PATH = os.path.join(os.path.dirname(__file__), os.path.pardir, 'procgen_wrapper',
                                'leaper_steps_per_seed.csv')
# This dictionary contains the number of steps to reach the goal for 1000 level seeds, generated by passing seeds 0-999
# to ProcgenSimulator
STEPS_PER_SEED = pd.read_csv(LEAPER_DATA_PATH, index_col=0)['Steps to Goal'].to_dict()


class AStarNode(BFSNode):
    def __init__(self, state, parent=None, action_from_parent=None, g=0., h=0., hash=None):
        super().__init__(state, parent)
        self.action = action_from_parent
        if hash:
            self.hash = hash
        self.g = g  # Cost from start node to current node
        self.h = h  # Heuristic (estimated cost from current node to end node)

    @property
    def f(self):
        return self.g + self.h

    @property
    def leading_actions(self) -> List[ProcgenAction]:
        node = self
        actions = []
        while node.parent:
            actions.insert(0, node.action)
            node = node.parent

        return actions

    def __lt__(self, other):
        return self.f < other.f


def astar(start_state: ExtendedState, simulator: ProcgenSimulator, timeout=1000):
    """
    A* search algorithm, with a timeout on maximum iterations.

    :param start_state: The starting state of the search
    :param simulator: The simulator object to use for getting state neighbors
    :param timeout: The maximum number of iterations to run the search
    """
    open_set = []
    closed_set = set()

    # Extract the level seed from the simulator, it applies to all states in the search
    start_state_info = simulator.get_info(start_state)
    level_seed = start_state_info[0]['level_seed']
    # Extract the number of steps to goal for this instance, it applies to all states in the search
    steps_to_goal = STEPS_PER_SEED[level_seed]
    start_state_agent_y = start_state.agent_y

    # Calculate the starting step heuristic
    h_0 = (steps_to_goal - start_state_agent_y) * -LEAPER_UP_PENALTY

    start_node = AStarNode(start_state, g=0, h=h_0)
    heapq.heappush(open_set, start_node)

    iter_num = 0

    while open_set:
        if iter_num > timeout:
            # In case of timeout, estimate the remaining steps to goal
            # Number of direct Up actions (regardless of obstacles)
            num_up_actions = steps_to_goal - start_state_agent_y
            # Estimate the number of non-Up actions based on the number of Up actions times 3
            num_steps = num_up_actions * NUM_STEPS_PER_UP_ACTION

            # Construct and return a dummy list of actions to yield the estimated penalties based on number of actions
            return [LeaperAction.Up] * num_up_actions + [LeaperAction.Right] * num_steps

        current_node = heapq.heappop(open_set)

        iter_num += 1

        if current_node.state.is_solved:
            return current_node.leading_actions

        closed_set.add(current_node.hash)

        for successor_state, cost, action in successors(current_node.state, simulator):
            hsh = hash_2d_array(successor_state.observation)
            if hsh in closed_set:
                continue

            g = current_node.g + cost
            h = (steps_to_goal - successor_state.agent_y) * -LEAPER_UP_PENALTY
            new_node = AStarNode(successor_state, parent=current_node, action_from_parent=action, g=g, h=h, hash=hsh)

            if any([node == new_node and node.f <= new_node.f for node in open_set]):
                # Don't add nodes that we already have their state (i.e., game state) in the open set with a lower cost
                continue

            heapq.heappush(open_set, new_node)


def successors(state, simulator):
    # Define successors for a state
    states = []
    for a in simulator.get_actions():
        next_state, _, _, info = simulator.step(state, a)
        cost = (-LEAPER_UP_PENALTY if a == LeaperAction.Up else -LEAPER_STEP_PENALTY)
        if not next_state.is_terminal or next_state.is_solved:
            states.append((next_state, cost, a))
    return states
